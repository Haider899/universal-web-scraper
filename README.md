ğŸŒ Universal Web Scraper Pro




Universal Web Scraper Pro is a powerful, flexible web scraping tool designed to work on any website.
It offers an interactive menu, multiple scraping modes, and exports data in professional formats â€” no coding required.

âš ï¸ Use responsibly. Respect website terms of service and robots.txt.

âœ¨ Key Features

ğŸŒ Universal Compatibility â€” Works on almost any website

ğŸ›ï¸ Interactive Menu System â€” Beginner-friendly, no coding needed

ğŸ•·ï¸ Multiple Scraping Modes â€” Single page, full site crawl, batch URLs

ğŸ§  Smart Data Extraction â€” Text, images, links, emails & more

ğŸ“¤ Multiple Export Formats â€” JSON, CSV, Excel

ğŸ›¡ï¸ Respectful Scraping â€” Rate limiting & robots.txt compliance

ğŸ”„ Advanced Error Handling â€” Automatic retries & recovery

ğŸš€ Quick Start
# Clone the repository
git clone https://github.com/Haider899/universal-web-scraper.git
cd universal-web-scraper

# Install dependencies
pip install -r requirements.txt

# Run the scraper
python advanced_scraper.py

ğŸ“¦ Requirements

Python 3.8+

pip (Python package manager)

Required libraries (installed automatically):

requests â€” HTTP requests

beautifulsoup4 â€” HTML parsing

pandas â€” Data processing & export

openpyxl â€” Excel support

lxml â€” Fast HTML parsing

ğŸ¯ Usage
ğŸ”¹ Interactive Mode (Recommended)
python advanced_scraper.py


Youâ€™ll see an interactive menu:

ğŸŒ==================================================ğŸŒ
           UNIVERSAL WEB SCRAPER PRO
ğŸŒ==================================================ğŸŒ

ğŸ“‹ MAIN MENU:
1. ğŸ¯ Scrape Single URL
2. ğŸ•·ï¸  Crawl Entire Website
3. ğŸ“ Scrape Multiple URLs
4. âš™ï¸  Settings & Configuration
5. ğŸ§ª Test Popular Websites
6. ğŸ“Š View Previous Results
7. âŒ Exit

ğŸ”¹ Single URL Scraping

Choose Option 1

Enter target URL (e.g. https://example.com)

Monitor real-time progress

Select export format (JSON / CSV / Excel)

Results saved automatically

ğŸ§© Python API Usage
from universal_scraper import UniversalScraper

scraper = UniversalScraper(base_delay=2)

data = scraper.scrape_url("https://example.com")

scraper.export_data(
    {'result': data},
    filename='my_report',
    formats=['json', 'csv']
)

ğŸªŸ Windows Installation
ğŸš€ Easy Method (Recommended)

Download ZIP (Code â†’ Download ZIP)

Extract to Desktop

Double-click install_and_run.bat

Follow on-screen instructions

ğŸ”§ Manual Method
cd Desktop\universal-web-scraper
pip install -r requirements.txt
python advanced_scraper.py

ğŸ§ Linux / macOS Installation
# Ubuntu / Debian
sudo apt update
sudo apt install python3 python3-pip

# macOS
brew install python

pip3 install -r requirements.txt
python3 advanced_scraper.py

ğŸ’¡ Examples
Batch Scraping Example
from universal_scraper import UniversalScraper

urls = ["https://site1.com", "https://site2.com"]
scraper = UniversalScraper()

results = {url: scraper.scrape_url(url) for url in urls}

scraper.export_data(results, 'batch_results', ['excel'])

â“ Troubleshooting
Python Not Found

Windows: Reinstall Python and check Add Python to PATH

Linux: sudo apt install python3

macOS: brew install python

Module Not Found
pip install requests beautifulsoup4 pandas openpyxl lxml

ğŸ¤ Contributing

Contributions are welcome!

git checkout -b feature/AmazingFeature
git commit -m "Add AmazingFeature"
git push origin feature/AmazingFeature


Open a Pull Request ğŸš€

ğŸ“„ License

This project is licensed under the MIT License.
See the LICENSE
 file for details.

ğŸ™ Acknowledgments

BeautifulSoup4 â€” HTML parsing

Requests â€” HTTP operations

Pandas â€” Data processing & export

â­ Support the Project

If you find this tool useful, please star the repository â­
It helps the project grow and motivates further development.
